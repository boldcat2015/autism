{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scraping Q&As of Li Fei from Haodf.com\n",
    "\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# weburl = \"http://www.haodf.com/wenda/dflifei_g_4642049195.htm\" # a test page\n",
    "webheader = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) \\\n",
    "        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36'}\n",
    "urls = []\n",
    "data = []\n",
    "\n",
    "def getSinglePage(pageurl):\n",
    "    req = urllib.request.Request(url=pageurl, headers=webheader)\n",
    "    try:\n",
    "        webPage = urllib.request.urlopen(req)\n",
    "        \"\"\"\n",
    "        print(type(webPage))\n",
    "        print(webPage.geturl())\n",
    "        print(webPage.info())\n",
    "        print(webPage.getcode())\n",
    "        \"\"\"\n",
    "    except:\n",
    "        print('\\033[33m Warning:\\033[0m Fail to open:', pageurl)\n",
    "        pass # failed +1\n",
    "        return None\n",
    "    else:\n",
    "        bsObj = BeautifulSoup(webPage, \"lxml\")\n",
    "        # questions = bsObj.findAll(\"div\", {\"class\":\"h_s_cons_info\"})\n",
    "        # answers = bsObj.findAll(\"div\", {\"class\":\"h_s_cons_docs\"})\n",
    "        patient = bsObj.find(lambda tag: tag.get_text() == '病情描述：')\n",
    "        if patient:\n",
    "            symptom = ''.join(list(map(str, patient.next_siblings))).replace('<br/>', '')\n",
    "            # get rid of bingli attachment\n",
    "            symptom = re.sub(r'<[^>]*>[^<>]*</[^>]*>|\\n|\\r', '', symptom)\n",
    "            patient = patient.parent.parent.parent.parent.parent # any better solution?\n",
    "            symptom_time = patient.find(\"div\", {\"class\":\"yh_l_times\"}).get_text()\n",
    "            for reply in patient.next_siblings: # diagnosis follows symptom\n",
    "                try:\n",
    "                    diagnosis = reply.find(\"div\", {\"class\":\"h_s_cons_docs\"})\n",
    "                    diagnosis = diagnosis.find(\"h3\").get_text()\n",
    "                except:\n",
    "                    continue # not a reply by doctor, keep searching...\n",
    "                else: # valid Q&A completes here\n",
    "                    diagnosis_time = reply.find(\"div\", {\"class\":\"yh_l_times\"}).get_text()\n",
    "                    \"\"\"\n",
    "                    print(symptom)\n",
    "                    print(symptom_time)\n",
    "                    print(diagnosis)\n",
    "                    print(diagnosis_time)\n",
    "                    \"\"\"\n",
    "                    data.append({'url':pageurl, \\\n",
    "                                 'symptom':symptom, 'symptom_time':symptom_time, \\\n",
    "                                 'diagnosis':diagnosis, 'diagnosis_time':diagnosis_time})\n",
    "                    break # only the first reply is valid\n",
    "        else: # not Q&As of desired pattern (see README.md)\n",
    "            pass # dirty data +1\n",
    "\n",
    "# getSinglePage(weburl)\n",
    "\n",
    "for i in range(1,97): # 1-96 pages, each has 25 posts\n",
    "    listurl = ''.join(['http://dflifei.haodf.com/zixun/list.htm?type=&p=',str(i)])\n",
    "    req = urllib.request.Request(url=listurl, headers=webheader)\n",
    "    listPage = urllib.request.urlopen(req)\n",
    "    print('Now scraping page', i)\n",
    "    bsObj = BeautifulSoup(listPage, \"lxml\")\n",
    "    listtable = bsObj.find(\"div\", {\"class\":\"zixun_list\"})\n",
    "    for post in listtable.findAll(\"a\", {\"class\":\"td_link\"}):\n",
    "        urls.append(post['href'])\n",
    "        print('    Now scraping post', post['href'])\n",
    "        getSinglePage(post['href'])\n",
    "        print(len(data), '/', len(urls)) # timely feedback makes me happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Scraped', len(data), 'valid data out of', len(urls), 'posts.')\n",
    "frame = pd.DataFrame(data)\n",
    "frame.to_pickle('../data/data-' \\\n",
    "                + datetime.datetime.today().strftime('%Y-%m-%d-%H.%M.%S') \\\n",
    "                + '.pickle')\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in frame.columns:\n",
    "    print(len(frame[frame[col] == '']), '/', len(frame), 'missing entries in column', col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
